name: CI/CD Pipeline

on:
  push:
    branches: [ main, develop ]
  pull_request:
    branches: [ main, develop ]
  workflow_dispatch:

concurrency:
  group: ${{ github.workflow }}-${{ github.ref }}
  cancel-in-progress: true

env:
  PYTHON_DEFAULT_VERSION: "3.11"

jobs:
  # Fast checks that run first
  pre-checks:
    name: Pre-flight Checks
    runs-on: ubuntu-latest
    timeout-minutes: 10
    outputs:
      python-versions: ${{ steps.python-versions.outputs.versions }}
    
    steps:
      - name: Checkout
        uses: actions/checkout@v4

      - name: Install uv
        uses: astral-sh/setup-uv@v6
        with:
          version: "latest"
          enable-cache: true
          python-version: ${{ env.PYTHON_DEFAULT_VERSION }}

      - name: Install dependencies
        run: uv sync --all-extras --dev

      - name: Determine Python versions to test
        id: python-versions
        run: |
          echo "versions=[\"3.10\", \"3.11\", \"3.12\", \"3.13\"]" >> $GITHUB_OUTPUT

      - name: Lint with Ruff
        run: |
          # Fail if there are any linting issues (no --fix in CI)
          uv run ruff check --output-format=github .
          
      - name: Format check with Ruff  
        run: |
          # Fail if files are not properly formatted (no auto-format in CI)
          uv run ruff format --check --diff .

      - name: Type check with MyPy
        run: |
          # Use same configuration as pre-commit for consistency
          uv run mypy --config-file=pyproject.toml

      - name: Security check
        run: |
          uv add --dev bandit[toml] safety
          uv run bandit -r src/ -f json
          uv run safety check --json

  # Core testing matrix
  test:
    name: Test (Python ${{ matrix.python-version }}, ${{ matrix.os }})
    runs-on: ${{ matrix.os }}
    needs: pre-checks
    timeout-minutes: 30
    strategy:
      fail-fast: false
      matrix:
        os: [ ubuntu-latest, windows-latest, macos-latest ]
        python-version: ${{ fromJson(needs.pre-checks.outputs.python-versions) }}
        exclude:
          # Skip some combinations to reduce CI time
          - os: windows-latest
            python-version: "3.10"
          - os: macos-latest
            python-version: "3.10"

    steps:
      - name: Checkout
        uses: actions/checkout@v4

      - name: Install UV
        uses: astral-sh/setup-uv@v6
        with:
          enable-cache: true
          python-version: ${{ matrix.python-version }}

      - name: Install dependencies
        run: uv sync --all-extras --dev

      - name: Run tests with coverage
        run: uv run pytest --cov=fastflight --cov-report=xml --cov-report=term --cov-branch --cov-fail-under=50 --junit-xml=pytest.xml -v

      - name: Upload coverage to Codecov
        if: matrix.os == 'ubuntu-latest' && matrix.python-version == env.PYTHON_DEFAULT_VERSION
        uses: codecov/codecov-action@v5
        with:
          fail_ci_if_error: false
          token: ${{ secrets.CODECOV_TOKEN }}
          slug: cning112/fastflight

  # Arrow Flight specific tests
  integration-tests:
    name: Arrow Flight Integration Tests
    runs-on: ubuntu-latest
    needs: pre-checks
    timeout-minutes: 20
    services:
      # We might need a test database or other services
      postgres:
        image: postgres:15
        env:
          POSTGRES_PASSWORD: postgres
          POSTGRES_DB: testdb
        options: >-
          --health-cmd pg_isready
          --health-interval 10s
          --health-timeout 5s
          --health-retries 5
        ports:
          - 5432:5432

    steps:
      - name: Checkout
        uses: actions/checkout@v4

      - name: Install UV
        uses: astral-sh/setup-uv@v6
        with:
          enable-cache: true
          python-version: ${{ env.PYTHON_DEFAULT_VERSION }}

      - name: Install dependencies
        run: uv sync --all-extras --dev

      - name: Start Flight server for testing
        run: |
          cd examples/multi_protocol_demo
          
          # Start FastFlight server using example script
          uv run python start_flight_server.py &
          FLIGHT_PID=$!
          echo "FLIGHT_PID=$FLIGHT_PID" >> $GITHUB_ENV
          
          # Start REST server using example script  
          uv run python start_rest_server.py &
          REST_PID=$!
          echo "REST_PID=$REST_PID" >> $GITHUB_ENV
          
          # Wait for servers to be ready
          for i in {1..30}; do
            if nc -z localhost 8815 && nc -z localhost 8000; then
              echo "Both servers are ready"
              break
            fi
            echo "Waiting for servers... ($i/30)"
            sleep 2
          done

      - name: Run integration tests
        run: |
          # Run the multi-protocol demo as integration test
          cd examples/multi_protocol_demo
          timeout 60 uv run python run_demo.py || echo "Demo completed or timed out"
        env:
          FLIGHT_SERVER_URL: "grpc://localhost:8815"
          POSTGRES_URL: "postgresql://postgres:postgres@localhost:5432/testdb"

      - name: Stop servers
        if: always()
        run: |
          if [ ! -z "$FLIGHT_PID" ]; then
            kill $FLIGHT_PID || true
          fi
          if [ ! -z "$REST_PID" ]; then
            kill $REST_PID || true
          fi

  # Performance benchmarks
  benchmarks:
    name: Performance Benchmarks
    runs-on: ubuntu-latest
    needs: pre-checks
    if: github.event_name == 'pull_request'
    timeout-minutes: 15

    steps:
      - name: Checkout
        uses: actions/checkout@v4

      - name: Install UV
        uses: astral-sh/setup-uv@v6
        with:
          enable-cache: true
          python-version: ${{ env.PYTHON_DEFAULT_VERSION }}

      - name: Install dependencies
        run: uv sync --all-extras --dev

      - name: Install benchmark dependencies
        run: |
          uv add --dev pytest-benchmark asv

      - name: Run benchmarks
        run: |
          uv run pytest tests/benchmarks/ --benchmark-json=benchmark.json

      - name: Store benchmark result
        uses: benchmark-action/github-action-benchmark@v1
        with:
          tool: 'pytest'
          output-file-path: benchmark.json
          github-token: ${{ secrets.GITHUB_TOKEN }}
          comment-on-alert: true
          alert-threshold: '200%'
          fail-on-alert: true

  # Build and validate package
  build:
    name: Build Package
    runs-on: ubuntu-latest
    needs: [ pre-checks, test ]
    timeout-minutes: 10

    steps:
      - name: Checkout
        uses: actions/checkout@v4

      - name: Install UV
        uses: astral-sh/setup-uv@v6
        with:
          enable-cache: true
          python-version: ${{ env.PYTHON_DEFAULT_VERSION }}

      - name: Build package
        run: uv build

      - name: Verify package
        run: |
          uv run twine check dist/*
          
          # Test installation in fresh environment
          uv venv test-env
          source test-env/bin/activate
          pip install dist/*.whl
          python -c "import fastflight; print(f'FastFlight version: {fastflight.__version__}')"

      - name: Upload build artifacts
        uses: actions/upload-artifact@v4
        with:
          name: dist-files
          path: dist/
          retention-days: 30

  # Documentation checks
  docs:
    name: Documentation
    runs-on: ubuntu-latest
    needs: pre-checks
    timeout-minutes: 10

    steps:
      - name: Checkout
        uses: actions/checkout@v4

      - name: Install UV
        uses: astral-sh/setup-uv@v6
        with:
          enable-cache: true
          python-version: ${{ env.PYTHON_DEFAULT_VERSION }}

      - name: Install dependencies
        run: |
          uv sync --all-extras --dev
          uv add --dev mkdocs mkdocs-material mkdocstrings[python]

      - name: Build documentation
        run: |
          uv run mkdocs build --strict

      - name: Upload docs artifacts
        uses: actions/upload-artifact@v4
        with:
          name: docs
          path: site/

  # Final status check
  ci-success:
    name: CI Success
    runs-on: ubuntu-latest
    needs: [ pre-checks, test, integration-tests, build, docs ]
    if: always()

    steps:
      - name: Check all jobs
        run: |
          if [[ "${{ needs.pre-checks.result }}" != "success" ]]; then
            echo "Pre-checks failed"
            exit 1
          fi
          if [[ "${{ needs.test.result }}" != "success" ]]; then
            echo "Tests failed"
            exit 1
          fi
          if [[ "${{ needs.integration-tests.result }}" != "success" ]]; then
            echo "Integration tests failed"
            exit 1
          fi
          if [[ "${{ needs.build.result }}" != "success" ]]; then
            echo "Build failed"
            exit 1
          fi
          if [[ "${{ needs.docs.result }}" != "success" ]]; then
            echo "Documentation build failed"
            exit 1
          fi
          echo "All checks passed!"
